# Prompt Engineering for In-Context Learning  

## üå∏ Project Overview
This project investigates the capabilities of large language models (LLMs) to perform diverse software engineering tasks through **in-context learning**. The core focus was on how different **prompting strategies** influence model behavior and output quality.

- **Prompt strategies**: Zero-Shot, Few-Shot, Chain-of-Thought, Prompt Chaining, Self-Consistency
- **LLMs used**: GPT-4 (ChatGPT), Gemini (Google)

## üìÅ Repository Contents

- `gemini_outputs/`  
  - Raw prompts and model responses for Gemini
 
- `gpt_outputs/`  
  - Raw prompts and model responses for GPT-4

- `scripts/` 
  ‚Äì Automation scripts for gpt; requires OpenAI subscription to run

- `GenAi_Project_3_Report.pdf` ‚Äì Main report with prompt comparison and task analysis  
- `Bonus_Questions_Report.pdf` ‚Äì Extra credit report (Tasks 21 & 22)  
- `README.md` ‚Äì You are here! :)
